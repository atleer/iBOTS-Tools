{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install ipympl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import caiman as cm\n",
    "import matplotlib.pyplot as plt\n",
    "from caiman.motion_correction import MotionCorrect\n",
    "from caiman.source_extraction.cnmf import cnmf\n",
    "from caiman.source_extraction.cnmf.params import CNMFParams\n",
    "from caiman.utils.visualization import view_quilt\n",
    "import pandas as pd\n",
    "import sciebo\n",
    "import bokeh.plotting as bpl\n",
    "import holoviews as hv\n",
    "bpl.output_notebook()\n",
    "hv.notebook_extension('bokeh')\n",
    "import holoviews as hv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sciebo.download_file_from_sciebo('https://uni-bonn.sciebo.de/s/RR7qj7tklW1rX25', 'data', 'Sue_2x_3000_40_-46.tif')\n",
    "sciebo.download_file_from_sciebo('https://uni-bonn.sciebo.de/s/RR7qj7tklW1rX25', 'data', 'data_endoscope.tif')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Before component evaluation, we need to run motion correction and source extraction\n",
    "\n",
    "We will do the initial steps using the parameters given directly in the demo notebook of Caiman. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading movie\n",
    "fname = \"data/Sue_2x_3000_40_-46.tif\"\n",
    "movie_orig = cm.load(fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparing parameters for component evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# general dataset-dependent parameters\n",
    "fr = 30                     # imaging rate in frames per second\n",
    "decay_time = 0.4            # length of a typical transient in seconds\n",
    "dxy = (2., 2.)              # spatial resolution in x and y in (um per pixel)\n",
    "\n",
    "# motion correction parameters\n",
    "strides = (48, 48)          # start a new patch for pw-rigid motion correction every x pixels\n",
    "overlaps = (24, 24)         # overlap between patches (width of patch = strides+overlaps)\n",
    "max_shifts = (6,6)          # maximum allowed rigid shifts (in pixels)\n",
    "max_deviation_rigid = 3     # maximum shifts deviation allowed for patch with respect to rigid shifts\n",
    "pw_rigid = True             # flag for performing non-rigid motion correction\n",
    "\n",
    "# CNMF parameters for source extraction and deconvolution\n",
    "p = 1                       # order of the autoregressive system (set p=2 if there is visible rise time in data)\n",
    "gnb = 2                     # number of global background components (set to 1 or 2)\n",
    "merge_thr = 0.85            # merging threshold, max correlation allowed\n",
    "bas_nonneg = True           # enforce nonnegativity constraint on calcium traces (technically on baseline)\n",
    "rf = 15                     # half-size of the patches in pixels (patch width is rf*2 + 1)\n",
    "stride_cnmf = 10             # amount of overlap between the patches in pixels (overlap is stride_cnmf+1) \n",
    "K = 4                       # number of components per patch\n",
    "gSig = np.array([4, 4])     # expected half-width of neurons in pixels (Gaussian kernel standard deviation)\n",
    "gSiz = 2*gSig + 1           # Gaussian kernel width and hight\n",
    "method_init = 'greedy_roi'  # initialization method (if analyzing dendritic data see demo_dendritic.ipynb)\n",
    "ssub = 1                    # spatial subsampling during initialization \n",
    "tsub = 1                    # temporal subsampling during intialization\n",
    "\n",
    "# parameters for component evaluation\n",
    "min_SNR = 2.0               # signal to noise ratio for accepting a component\n",
    "rval_thr = 0.85             # space correlation threshold for accepting a component\n",
    "cnn_thr = 0.99              # threshold for CNN based classifier\n",
    "cnn_lowest = 0.1            # neurons with cnn probability lower than this value are rejected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameter_dict = {'fnames': fname,\n",
    "                  'fr': fr,\n",
    "                  'dxy': dxy,\n",
    "                  'decay_time': decay_time,\n",
    "                  'strides': strides,\n",
    "                  'overlaps': overlaps,\n",
    "                  'max_shifts': max_shifts,\n",
    "                  'max_deviation_rigid': max_deviation_rigid,\n",
    "                  'pw_rigid': pw_rigid,\n",
    "                  'p': p,\n",
    "                  'nb': gnb,\n",
    "                  'rf': rf,\n",
    "                  'K': K, \n",
    "                  'gSig': gSig,\n",
    "                  'gSiz': gSiz,\n",
    "                  'stride': stride_cnmf,\n",
    "                  'method_init': method_init,\n",
    "                  'rolling_sum': True,\n",
    "                  'only_init': True,\n",
    "                  'ssub': ssub,\n",
    "                  'tsub': tsub,\n",
    "                  'merge_thr': merge_thr, \n",
    "                  'bas_nonneg': bas_nonneg,\n",
    "                  'min_SNR': min_SNR,\n",
    "                  'rval_thr': rval_thr,\n",
    "                  'use_cnn': True,\n",
    "                  'min_cnn_thr': cnn_thr,\n",
    "                  'cnn_lowest': cnn_lowest}\n",
    "\n",
    "parameters = CNMFParams(params_dict=parameter_dict) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mc = MotionCorrect(fname, **parameters.motion)\n",
    "mc.motion_correct(save_movie=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mc_fname = cm.save_memmap(\n",
    "    mc.fname_tot_els,\n",
    "    base_name='memmap_',\n",
    "    order='C'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Yr, dims, num_frames = cm.load_memmap(mc_fname)\n",
    "images = np.reshape(Yr.T, [num_frames] + list(dims), order='F')\n",
    "images.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNMF Model fitting and re-fitting\n",
    "\n",
    "Below steps can take a while but not as long as before as we have set a lot of parameters: You can listen to this [Music](https://www.youtube.com/watch?v=HImi4zdoZrM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnmf_model = cnmf.CNMF(n_processes=1, params=parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnmf_fit = cnmf_model.fit(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnmf_refit = cnmf_fit.refit(images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNMF Model evaluation\n",
    "\n",
    "\n",
    "**Signal to Noise Ratio (SNR) `min_SNR`**\n",
    "\n",
    "What it is: This is a measure of how much the signal (in this case, the activity of neurons as indicated by calcium transients) stands out from the background noise. </br>\n",
    "How it's done: For each neuron's activity trace (a graph showing their activity over time), a baseline level of noise is determined. Then, the SNR calculates how much the peaks of activity (the calcium traces) stand out compared to this baseline noise. </br>\n",
    "Why it matters: Higher SNR means the neuron's activity is clearer and more distinct, making it more reliable. High SNR components are considered high quality and are less likely to be mistaken detections (false positives). </br>\n",
    "\n",
    "**Spatial Correlation `rval_thr`**\n",
    "\n",
    "What it is: This checks how well the shapes and locations of neurons (their \"spatial footprints\") detected in the data match up with where and when actual neuron activity is seen in the video (movie) of the brain. </br>\n",
    "How it's done: The spatial footprints extracted are compared to the actual neuron activity in the video. This comparison generates correlation coefficients (values that measure how similar two patterns are) for the times when the neurons are active. </br>\n",
    "Why it matters: High correlation means the detected spatial footprints accurately represent real neuron activity, which is crucial for valid analysis. </br>\n",
    "\n",
    "\n",
    "**CNN Confidence `min_cnn_thr`**\n",
    "\n",
    "What it is: This uses a Convolutional Neural Network (CNN) to evaluate whether the detected shapes of neurons are likely to be real. </br>\n",
    "How it's done: Each detected spatial component (neuron shape) is analyzed by the CNN, which has been trained on a large set of data where the correct answers (which shapes are truly neurons) are already known. </br>\n",
    "Why it matters: The CNN gives a confidence score between 0 and 1 for each shape. Scores closer to 1 indicate the shape is very likely to be a real neuron. </br>\n",
    "\n",
    "\n",
    "\n",
    "These parameters can be set in the `quality` field of `params`. Let's see that they are set to the values we assigned before. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the min_SNR, rval_thr, min_cnn_thr used for evaluation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnmf_refit.params.quality['min_SNR']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnmf_refit.params.quality['blank'] # fill the blank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnmf_refit.params.quality['blank'] # fill the blank"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The evaluate_components() is the method that uses the min_SNR, rval_thr, and min_cnn_thr to `accept` or `reject` the identified components. If an identified component is below all of the three values, it will be rejected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnmf_refit.estimates.evaluate_components(images, cnmf_refit.params);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `evaluate_components` method creates `idx_components` (accepted) and `idx_components_bad` (rejected) fields in the `Estimates` class.\n",
    "\n",
    "Let's see how many accepted and rejected components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(cnmf_refit.estimates.idx_components)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many bad components?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the contours of both accepted and rejected components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_image = cm.local_correlations(images, swap_dim=False)\n",
    "cnmf_refit.estimates.plot_contours_nb(img=correlation_image, idx=cnmf_refit.estimates.idx_components)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are also some visualizations that you can use to visualize denoised calcium traces of each component and their SNR\n",
    "\n",
    "Accepted components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_image = cm.local_correlations(images, swap_dim=False)\n",
    "cnmf_refit.estimates.nb_view_components(img=correlation_image, idx=cnmf_refit.estimates.idx_components, cmap='gray', denoised_color='red');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bad components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Component evaluation of 1-photon data. The procedure is similar to the one we followed in the notebook. If you want some data for setting parameters, here is a link to the caiman tutorial on [CNMF-E Demo](https://github.com/flatironinstitute/CaImAn/blob/main/demos/notebooks/demo_pipeline_cnmfE.ipynb)\n",
    "\n",
    "2. Exercise: delta F/F\n",
    "\n",
    "Follow `Extract delta F/F` section of [CNMF Demo](https://github.com/flatironinstitute/CaImAn/blob/main/demos/notebooks/demo_pipeline.ipynb)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "caiman",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
